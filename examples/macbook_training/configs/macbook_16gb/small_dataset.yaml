# MacBook 16GB RAM - Small Dataset Configuration
# Optimized for datasets < 100MB on MacBooks with 16GB RAM

defaults:
  - arch: trm
  - _self_

# MacBook hardware profile
hardware_profile: "macbook_16gb"
expected_ram_gb: 16
expected_cpu_cores: 8

# Data configuration
data_paths: ['data/small-dataset']
data_paths_test: []

# Training parameters optimized for 16GB RAM
global_batch_size: 64  # Larger batch size with more RAM
effective_batch_size: 256  # Higher effective batch size

# Training configuration for 16GB RAM + small dataset
epochs: 2000  # Required field at top level
training:
  eval_interval: 200
  checkpoint_every_eval: true
  
  # Memory management with more headroom
  memory_limit_mb: 8000  # More generous limit
  gradient_accumulation_steps: 4
  max_sequence_length: 512  # Longer sequences possible
  
  # CPU optimization for 8-core CPU
  num_workers: 4  # More workers with more RAM
  pin_memory: true  # Can afford to pin memory
  enable_cpu_optimization: true
  torch_threads: 8
  
  # Checkpoint management
  checkpoint_interval: 500
  max_checkpoints_to_keep: 3
  
  # Progress monitoring
  monitoring_interval: 2.0
  enable_thermal_monitoring: true

# Learning rate for larger batch sizes
lr: 1e-4  # Standard learning rate
lr_min_ratio: 0.1
lr_warmup_steps: 500

# Regularization
beta1: 0.9
beta2: 0.95
weight_decay: 0.1
puzzle_emb_weight_decay: 0.1

# Puzzle embedding training
puzzle_emb_lr: 1e-2

# Model architecture taking advantage of more RAM
arch:
  # Required fields
  name: "trm"
  loss:
    name: "cross_entropy"
  
  # Larger model possible with 16GB RAM
  hidden_size: 384
  num_heads: 12
  L_layers: 3
  expansion: 4
  
  # Enhanced recursive reasoning
  H_cycles: 3
  L_cycles: 4
  halt_max_steps: 8
  halt_exploration_prob: 0.1
  
  # Training settings
  forward_dtype: "float32"
  embed_scale: true
  init_std: 1.0
  
  # Sequence parameters
  seq_len: 512
  batch_size: 16  # Larger physical batch size
  
  # RMS norm parameters
  rms_norm_eps: 1e-5

# Evaluators
evaluators:
  - name: arc@ARC

# System settings
seed: 42
min_eval_interval: 0
ema: true  # Can afford EMA with more memory
ema_rate: 0.999
freeze_weights: false

# Logging
use_wandb: true
wandb_project: "macbook-trm-16gb"
log_interval: 50

# Output configuration
output_dir: "outputs/macbook_16gb_small"
experiment_name: "trm_small_16gb"

# MacBook-specific optimizations for 16GB RAM
macbook_optimizations:
  # Relaxed memory management
  enable_memory_monitoring: true
  memory_pressure_threshold: 80.0
  dynamic_batch_sizing: true
  auto_garbage_collection: true
  
  # CPU optimization for 8-core CPU
  use_mkl: true
  optimize_tensor_operations: true
  enable_vectorization: true
  
  # Dataset management - can load in memory
  streaming_threshold_mb: 200.0
  cache_threshold_mb: 100.0
  chunk_size_mb: 50.0
  enable_caching: true
  auto_fallback_streaming: false  # Prefer in-memory loading
  
  # Thermal management
  thermal_throttle_threshold: 85.0
  cooling_delay_seconds: 1.0
  
  # Progress reporting
  show_resource_usage: true
  show_eta: true
  show_samples_per_second: true