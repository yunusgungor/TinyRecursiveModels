# Email Classification Training Configuration for 16GB MacBook
# Balanced configuration for moderate memory environments
# Requirements: 2.1, 2.2

# Model Architecture
model:
  name: "EmailTRM"
  vocab_size: 5000  # Standard vocabulary size
  hidden_size: 384  # Medium model size
  num_layers: 2
  num_email_categories: 10
  max_sequence_length: 512  # Full sequence length

# Training Parameters
training:
  batch_size: 4  # Moderate batch size
  gradient_accumulation_steps: 8  # Balanced accumulation
  learning_rate: 1e-4
  weight_decay: 0.01
  max_epochs: 12
  max_steps: 8000
  warmup_steps: 300
  
  # Optimization settings
  use_mixed_precision: false  # Conservative for compatibility
  gradient_checkpointing: true
  dataloader_num_workers: 2

# Email-Specific Configuration
email:
  use_email_structure: true
  subject_attention_weight: 2.0  # Standard weight
  pooling_strategy: "weighted"
  enable_subject_prioritization: true
  use_hierarchical_attention: true  # Enable for better performance
  email_augmentation_prob: 0.3  # Standard augmentation

# MacBook Optimization
hardware:
  memory_limit_mb: 12000  # Use ~75% of 16GB
  enable_memory_monitoring: true
  dynamic_batch_sizing: true
  use_cpu_optimization: true
  thermal_monitoring: true
  
  # CPU settings
  num_workers: 2
  cpu_threads: 6  # More threads available
  
  # Memory management
  garbage_collection_frequency: 200  # Less frequent GC
  checkpoint_memory_cleanup: true

# Performance Targets
targets:
  target_accuracy: 0.95  # Full target accuracy
  min_category_accuracy: 0.90
  early_stopping_patience: 5  # Standard patience

# Data Configuration
data:
  train_split: 0.8
  val_split: 0.2
  shuffle_buffer_size: 2000  # Larger buffer
  prefetch_factor: 2
  
  # Balanced loading
  streaming_mode: false  # Can load full dataset
  chunk_size: 1000
  cache_preprocessed: true  # Cache for speed

# Checkpointing
checkpointing:
  save_interval_steps: 800
  max_checkpoints: 3  # Keep more checkpoints
  save_optimizer_state: true  # Save full state
  compression: true

# Logging and Monitoring
logging:
  log_interval: 25
  eval_interval: 400
  save_interval: 800
  
  # Resource monitoring
  monitor_memory: true
  monitor_cpu: true
  monitor_temperature: true
  
  # Wandb settings
  use_wandb: true
  wandb_project: "email-classification-16gb"

# Training Strategy
strategy:
  type: "multi_phase"
  phases:
    - name: "warmup"
      steps: 1500
      learning_rate_factor: 0.5
      batch_size_factor: 0.75
    - name: "main"
      steps: 5000
      learning_rate_factor: 1.0
      batch_size_factor: 1.0
    - name: "fine_tune"
      steps: 1500
      learning_rate_factor: 0.1
      batch_size_factor: 0.75

# Error Handling
error_handling:
  max_retries: 3
  retry_delay: 3
  graceful_degradation: true
  auto_reduce_batch_size: true
  memory_pressure_threshold: 0.80

# Advanced Features
advanced:
  # Ensemble training (if memory allows)
  enable_ensemble: false
  ensemble_size: 2
  
  # Hyperparameter optimization
  enable_auto_tuning: false
  tuning_trials: 5
  
  # Model compression
  enable_pruning: false
  pruning_ratio: 0.1