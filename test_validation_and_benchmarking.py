#!/usr/bin/env python3
"""
Validation and Benchmarking Tests

This module implements task 8.4: Write validation and benchmarking tests
- Test accuracy validation methodology and metrics
- Validate robustness testing procedures and results
- Test performance benchmarking and report generation
"""

import os
import json
import tempfile
import unittest
from unittest.mock import Mock, patch, MagicMock
from pathlib import Path
import numpy as np
from datetime import datetime

# Import the systems we're testing
from accuracy_validation_system import (
    AccuracyValidationSystem, AccuracyValidationConfig, 
    ValidationExperimentResult, ComprehensiveValidationResult
)
from robustness_testing_system import (
    RobustnessTestingSystem, RobustnessTestConfig,
    EmailVariationGenerator, RobustnessTestResult, ComprehensiveRobustnessResult
)
from performance_benchmarking_system import (
    PerformanceBenchmarkingSystem, BaselineModelImplementation,
    PerformanceMetrics, BenchmarkingResult
)


class TestAccuracyValidationSystem(unittest.TestCase):
    """Test cases for accuracy validation system."""
    
    def setUp(self):
        """Set up test fixtures."""
        self.temp_dir = tempfile.mkdtemp()
        self.config = AccuracyValidationConfig(
            dataset_paths=["test_dataset"],
            training_strategies=["multi_phase"],
            max_steps_per_experiment=100,  # Small for testing
            num_validation_runs=1,
            model_configs=[{"hidden_size": 256, "num_layers": 2}],
            test_languages=["en"],
            output_dir=self.temp_dir
        )
        self.validation_system = AccuracyValidationSystem(self.config)
    
    def tearDown(self):
        """Clean up test fixtures."""
        import shutil
        shutil.rmtree(self.temp_dir, ignore_errors=True)
    
    def test_validation_config_initialization(self):
        """Test validation configuration initialization."""
        self.assertEqual(len(self.config.training_strategies), 1)
        self.assertEqual(self.config.training_strategies[0], "multi_phase")
        self.assertEqual(self.config.target_accuracy, 0.95)
        self.assertEqual(self.config.min_category_accuracy, 0.90)
        self.assertTrue(Path(self.config.output_dir).exists())
    
    def test_validation_config_defaults(self):
        """Test validation configuration default values."""
        default_config = AccuracyValidationConfig(dataset_paths=["test"])
        
        self.assertIsNotNone(default_config.training_strategies)
        self.assertIsNotNone(default_config.model_configs)
        self.assertIsNotNone(default_config.test_languages)
        self.assertIn("multi_phase", default_config.training_strategies)
        self.assertIn("progressive", default_config.training_strategies)
    
    def test_experiment_configuration_generation(self):
        """Test generation of experiment configurations."""
        configs = self.validation_system._generate_experiment_configurations()
        
        # Should have configs for each combination
        expected_count = (
            len(self.config.dataset_paths) * 
            len(self.config.training_strategies) * 
            len(self.config.model_configs) * 
            len(self.config.test_languages) * 
            self.config.num_validation_runs
        )
        
        self.assertEqual(len(configs), expected_count)
        
        # Check first config structure
        config = configs[0]
        self.assertIn('dataset_path', config)
        self.assertIn('training_strategy', config)
        self.assertIn('model_config', config)
        self.assertIn('language', config)
        self.assertIn('run_index', config)
    
    def test_training_config_creation(self):
        """Test training configuration creation."""
        model_config = {"hidden_size": 512, "num_layers": 3, "learning_rate": 1e-4}
        training_config = self.validation_system._create_training_config(model_config)
        
        self.assertEqual(training_config.hidden_size, 512)
        self.assertEqual(training_config.num_layers, 3)
        self.assertEqual(training_config.learning_rate, 1e-4)
        self.assertEqual(training_config.target_accuracy, 0.95)
        self.assertEqual(training_config.num_email_categories, 10)
    
    def test_sample_test_data(self):
        """Test test data sampling."""
        # Create mock dataset
        dataset = []
        categories = ["Newsletter", "Work", "Personal", "Spam"]
        
        for i in range(100):
            dataset.append({
                'subject': f'Test email {i}',
                'body': f'This is test email content {i}',
                'category': categories[i % len(categories)],
                'category_id': i % len(categories)
            })
        
        sampled = self.validation_system._sample_test_data(dataset)
        
        # Should sample requested number or less
        self.assertLessEqual(len(sampled), self.config.samples_per_test)
        
        # Should have representation from different categories
        sampled_categories = set(sample['category'] for sample in sampled)
        self.assertGreater(len(sampled_categories), 1)
    
    def test_validation_result_analysis(self):
        """Test validation result analysis."""
        # Create mock experiment results
        mock_results = []
        
        for i in range(5):
            result = ValidationExperimentResult(
                experiment_id=f"test_exp_{i}",
                timestamp=datetime.now(),
                dataset_path="test_dataset",
                training_strategy="multi_phase",
                model_config={"hidden_size": 256},
                language="en",
                training_result=None,
                training_success=True,
                training_time=100.0,
                evaluation_result=None,
                evaluation_success=True,
                overall_accuracy=0.92 + i * 0.01,  # Varying accuracy
                category_accuracies={"Newsletter": 0.9, "Work": 0.95, "Personal": 0.88},
                min_category_accuracy=0.88,
                accuracy_target_met=i >= 3,  # Some meet target
                category_target_met=True,
                inference_time_ms=50.0,
                throughput_samples_per_sec=20.0,
                confidence_calibration_error=0.05,
                prediction_uncertainty=0.1,
                errors=[],
                warnings=[]
            )
            mock_results.append(result)
        
        self.validation_system.experiment_results = mock_results
        
        # Analyze results
        comprehensive_result = self.validation_system._analyze_validation_results(0.0)
        
        self.assertEqual(comprehensive_result.total_experiments, 5)
        self.assertEqual(comprehensive_result.successful_experiments, 5)
        self.assertEqual(comprehensive_result.experiments_meeting_both_targets, 2)  # Last 2 meet accuracy target
        self.assertGreater(comprehensive_result.best_overall_accuracy, 0.95)
        self.assertIsNotNone(comprehensive_result.validation_summary)
        self.assertIsInstance(comprehensive_result.recommendations, list)
    
    @patch('accuracy_validation_system.EmailTrainingOrchestrator')
    @patch('accuracy_validation_system.EmailTokenizer')
    def test_single_experiment_execution(self, mock_tokenizer, mock_orchestrator):
        """Test single experiment execution with mocks."""
        # Mock the orchestrator
        mock_training_result = Mock()
        mock_training_result.success = True
        mock_training_result.model_path = "test_model.pt"
        mock_training_result.errors = []
        mock_training_result.warnings = []
        
        mock_orchestrator_instance = Mock()
        mock_orchestrator_instance.execute_training_pipeline.return_value = mock_training_result
        mock_orchestrator.return_value = mock_orchestrator_instance
        
        # Mock model loading and evaluation
        with patch.object(self.validation_system, '_load_trained_model') as mock_load_model, \
             patch.object(self.validation_system, '_create_test_dataloader') as mock_create_loader, \
             patch.object(self.validation_system.evaluator, 'evaluate_model') as mock_evaluate:
            
            mock_model = Mock()
            mock_load_model.return_value = mock_model
            
            mock_dataloader = Mock()
            mock_create_loader.return_value = mock_dataloader
            
            mock_eval_result = Mock()
            mock_eval_result.overall_accuracy = 0.96
            mock_eval_result.category_metrics = {
                "Newsletter": Mock(f1_score=0.95),
                "Work": Mock(f1_score=0.97)
            }
            mock_eval_result.expected_calibration_error = 0.03
            mock_eval_result.prediction_entropy = 0.2
            mock_eval_result.inference_time_stats = {'mean_time_per_sample': 0.05}
            mock_eval_result.throughput_metrics = {'samples_per_second': 20.0}
            mock_evaluate.return_value = mock_eval_result
            
            # Execute experiment
            exp_config = {
                "dataset_path": "test_dataset",
                "training_strategy": "multi_phase",
                "model_config": {"hidden_size": 256},
                "language": "en",
                "experiment_name": "test_experiment"
            }
            
            result = self.validation_system._execute_single_experiment(exp_config)
            
            # Verify result
            self.assertTrue(result.training_success)
            self.assertTrue(result.evaluation_success)
            self.assertEqual(result.overall_accuracy, 0.96)
            self.assertTrue(result.accuracy_target_met)  # 0.96 > 0.95


class TestRobustnessTestingSystem(unittest.TestCase):
    """Test cases for robustness testing system."""
    
    def setUp(self):
        """Set up test fixtures."""
        self.temp_dir = tempfile.mkdtemp()
        self.config = RobustnessTestConfig(
            test_email_formats=True,
            test_adversarial=True,
            test_noise_robustness=True,
            samples_per_test=10,  # Small for testing
            output_dir=self.temp_dir
        )
        self.robustness_system = RobustnessTestingSystem(self.config)
    
    def tearDown(self):
        """Clean up test fixtures."""
        import shutil
        shutil.rmtree(self.temp_dir, ignore_errors=True)
    
    def test_robustness_config_initialization(self):
        """Test robustness configuration initialization."""
        self.assertTrue(self.config.test_email_formats)
        self.assertTrue(self.config.test_adversarial)
        self.assertTrue(self.config.test_noise_robustness)
        self.assertEqual(self.config.samples_per_test, 10)
        self.assertIsNotNone(self.config.format_variations)
        self.assertIsNotNone(self.config.adversarial_techniques)
    
    def test_email_variation_generator(self):
        """Test email variation generator."""
        generator = EmailVariationGenerator()
        
        # Test format variations
        email_text = "This is a test email about a meeting."
        subject = "Test Meeting"
        
        variations = generator.generate_format_variations(email_text, subject)
        
        self.assertIn('html', variations)
        self.assertIn('plain_text', variations)
        self.assertIn('forwarded', variations)
        
        # Check HTML format
        html_variation = variations['html']
        self.assertIn('<html>', html_variation['body'])
        self.assertIn('<body>', html_variation['body'])
        
        # Check forwarded format
        fwd_variation = variations['forwarded']
        self.assertTrue(fwd_variation['subject'].startswith('Fwd:'))
        self.assertIn('Forwarded message', fwd_variation['body'])
    
    def test_domain_variations(self):
        """Test domain-specific variations."""
        generator = EmailVariationGenerator()
        
        email_text = "Please review the attached document."
        subject = "Document Review"
        
        # Test corporate domain
        corporate_variation = generator.generate_domain_variations(email_text, subject, "corporate")
        self.assertIn("Dear Team", corporate_variation['body'])
        self.assertIn("Best regards", corporate_variation['body'])
        
        # Test personal domain
        personal_variation = generator.generate_domain_variations(email_text, subject, "personal")
        self.assertIn("Hi!", personal_variation['body'])
        
        # Test marketing domain
        marketing_variation = generator.generate_domain_variations(email_text, subject, "marketing")
        self.assertIn("üéâ", marketing_variation['body'])
        self.assertTrue(marketing_variation['subject'].startswith("üî•"))
    
    def test_adversarial_variations(self):
        """Test adversarial variation generation."""
        generator = EmailVariationGenerator()
        
        original_text = "This is an important meeting about the project."
        
        # Test typo generation
        typo_text = generator.generate_adversarial_variations(original_text, "typos")
        self.assertIsInstance(typo_text, str)
        self.assertGreater(len(typo_text), 0)
        
        # Test synonym replacement
        synonym_text = generator.generate_adversarial_variations(original_text, "synonyms")
        self.assertIsInstance(synonym_text, str)
        # Should replace "important" with synonym
        self.assertNotEqual(synonym_text, original_text)
        
        # Test spam evasion
        spam_text = generator.generate_adversarial_variations(original_text, "spam_evasion")
        self.assertIsInstance(spam_text, str)
        self.assertNotEqual(spam_text, original_text)
    
    def test_noise_variations(self):
        """Test noise variation generation."""
        generator = EmailVariationGenerator()
        
        original_text = "This is a clean email message for testing."
        
        # Test character noise
        char_noise = generator.generate_noise_variations(original_text, "character", 0.1)
        self.assertIsInstance(char_noise, str)
        self.assertNotEqual(char_noise, original_text)
        
        # Test word noise
        word_noise = generator.generate_noise_variations(original_text, "word", 0.2)
        self.assertIsInstance(word_noise, str)
        
        # Test sentence noise
        sentence_noise = generator.generate_noise_variations(original_text, "sentence", 0.1)
        self.assertIsInstance(sentence_noise, str)
    
    def test_length_variations(self):
        """Test length variation generation."""
        generator = EmailVariationGenerator()
        
        original_text = "This is a medium length email message for testing purposes and evaluation."
        
        # Test very short
        very_short = generator.generate_length_variations(original_text, "very_short")
        self.assertLess(len(very_short.split()), len(original_text.split()))
        
        # Test short
        short = generator.generate_length_variations(original_text, "short")
        self.assertLessEqual(len(short.split()), 25)
        
        # Test long
        long_text = generator.generate_length_variations(original_text, "long")
        self.assertGreater(len(long_text.split()), len(original_text.split()))
        
        # Test very long
        very_long = generator.generate_length_variations(original_text, "very_long")
        self.assertGreater(len(very_long.split()), len(long_text.split()))
    
    def test_sample_test_data(self):
        """Test test data sampling for robustness testing."""
        # Create mock dataset
        dataset = []
        for i in range(50):
            dataset.append({
                'subject': f'Test email {i}',
                'body': f'This is test email content {i}',
                'category': f'category_{i % 5}',
                'category_id': i % 5
            })
        
        sampled = self.robustness_system._sample_test_data(dataset)
        
        # Should sample requested number
        self.assertEqual(len(sampled), self.config.samples_per_test)
        
        # Should have samples from different categories
        categories = set(sample['category'] for sample in sampled)
        self.assertGreater(len(categories), 1)
    
    def test_robustness_result_analysis(self):
        """Test robustness result analysis."""
        # Create mock test results
        mock_results = []
        
        test_types = ["email_format", "adversarial", "noise"]
        variations = ["html", "typos", "character_0.1"]
        
        for i, (test_type, variation) in enumerate(zip(test_types, variations)):
            result = RobustnessTestResult(
                test_id=f"test_{i}",
                test_type=test_type,
                test_variation=variation,
                timestamp=datetime.now(),
                original_samples=10,
                modified_samples=10,
                original_accuracy=0.95,
                modified_accuracy=0.90 - i * 0.05,  # Decreasing accuracy
                accuracy_degradation=0.05 + i * 0.05,
                original_f1_macro=0.94,
                modified_f1_macro=0.89 - i * 0.05,
                f1_degradation=0.05 + i * 0.05,
                category_degradation={"Newsletter": 0.02, "Work": 0.03},
                most_affected_categories=[("Work", 0.03)],
                original_avg_confidence=0.85,
                modified_avg_confidence=0.80,
                confidence_degradation=0.05,
                new_errors=2,
                error_patterns={},
                failed_samples=[],
                robustness_score=0.9 - i * 0.1,
                passes_threshold=i == 0,  # Only first passes
                test_duration=10.0
            )
            mock_results.append(result)
        
        self.robustness_system.test_results = mock_results
        
        # Analyze results
        comprehensive_result = self.robustness_system._analyze_robustness_results(0.0)
        
        self.assertEqual(comprehensive_result.total_tests, 3)
        self.assertEqual(comprehensive_result.passed_tests, 1)  # Only first passes
        self.assertEqual(comprehensive_result.failed_tests, 2)
        self.assertLess(comprehensive_result.overall_robustness_score, 1.0)
        self.assertGreater(comprehensive_result.worst_case_degradation, 0.0)
        self.assertIsNotNone(comprehensive_result.robustness_summary)
    
    @patch('torch.no_grad')
    def test_evaluate_samples_mock(self, mock_no_grad):
        """Test sample evaluation with mocked PyTorch."""
        # Mock model and dataloader
        mock_model = Mock()
        mock_model.eval.return_value = None
        
        # Mock model output
        mock_logits = Mock()
        mock_logits.shape = (2, 10)  # 2 samples, 10 categories
        
        mock_outputs = {'logits': mock_logits}
        mock_model.return_value = mock_outputs
        
        # Mock dataloader
        mock_batch1 = {
            'inputs': Mock(),
            'labels': Mock()
        }
        mock_batch1['labels'].cpu.return_value.numpy.return_value = np.array([0, 1])
        
        mock_dataloader = [mock_batch1]
        mock_dataloader.batch_size = 2
        
        # Mock torch functions
        with patch('torch.argmax') as mock_argmax, \
             patch('torch.nn.functional.softmax') as mock_softmax, \
             patch('torch.max') as mock_max:
            
            # Setup mock returns
            mock_predictions = Mock()
            mock_predictions.cpu.return_value.numpy.return_value = np.array([0, 1])
            mock_argmax.return_value = mock_predictions
            
            mock_probabilities = Mock()
            mock_softmax.return_value = mock_probabilities
            
            mock_confidences = Mock()
            mock_confidences.cpu.return_value.numpy.return_value = np.array([0.9, 0.8])
            mock_max.return_value = (mock_confidences, None)
            
            # Execute evaluation
            result = self.robustness_system._evaluate_samples(mock_model, mock_dataloader)
            
            # Verify result structure
            self.assertIn('accuracy', result)
            self.assertIn('f1_macro', result)
            self.assertIn('avg_confidence', result)
            self.assertIn('category_accuracies', result)
            self.assertIn('error_indices', result)


class TestPerformanceBenchmarkingSystem(unittest.TestCase):
    """Test cases for performance benchmarking system."""
    
    def setUp(self):
        """Set up test fixtures."""
        self.temp_dir = tempfile.mkdtemp()
        self.benchmarking_system = PerformanceBenchmarkingSystem(
            output_dir=self.temp_dir
        )
    
    def tearDown(self):
        """Clean up test fixtures."""
        import shutil
        shutil.rmtree(self.temp_dir, ignore_errors=True)
    
    def test_baseline_model_implementation(self):
        """Test baseline model implementations."""
        category_names = ["Newsletter", "Work", "Personal", "Spam"]
        baseline_impl = BaselineModelImplementation(category_names)
        
        # Test random baseline
        random_baseline = baseline_impl.create_random_baseline()
        self.assertEqual(random_baseline['name'], "Random Baseline")
        self.assertIn('predict_function', random_baseline)
        self.assertEqual(random_baseline['training_time'], 0.0)
        
        # Test simple NN baseline
        simple_nn = baseline_impl.create_simple_nn_baseline()
        self.assertEqual(simple_nn['name'], "Simple NN Baseline")
        self.assertGreater(simple_nn['training_time'], 0.0)
        
        # Test transformer baseline
        transformer = baseline_impl.create_transformer_baseline()
        self.assertEqual(transformer['name'], "Transformer Baseline")
        self.assertGreater(transformer['memory_usage'], simple_nn['memory_usage'])
    
    def test_baseline_predictions(self):
        """Test baseline model predictions."""
        category_names = ["Newsletter", "Work", "Personal", "Spam"]
        baseline_impl = BaselineModelImplementation(category_names)
        
        # Create test samples
        test_samples = [
            {'subject': 'Newsletter Update', 'body': 'Weekly newsletter with updates'},
            {'subject': 'Meeting Tomorrow', 'body': 'Important work meeting scheduled'},
            {'subject': 'Free Offer!', 'body': 'Click here for free spam offer'},
            {'subject': 'Personal Note', 'body': 'Hey friend, how are you doing?'}
        ]
        
        # Test random baseline
        random_baseline = baseline_impl.create_random_baseline()
        predictions, confidences = random_baseline['predict_function'](test_samples)
        
        self.assertEqual(len(predictions), len(test_samples))
        self.assertEqual(len(confidences), len(test_samples))
        self.assertTrue(all(0 <= p < len(category_names) for p in predictions))
        self.assertTrue(all(0 <= c <= 1 for c in confidences))
        
        # Test simple NN baseline
        simple_nn = baseline_impl.create_simple_nn_baseline()
        predictions, confidences = simple_nn['predict_function'](test_samples)
        
        self.assertEqual(len(predictions), len(test_samples))
        # Should classify spam email as spam (category 3)
        spam_idx = 2  # "Free Offer!" email
        self.assertEqual(predictions[spam_idx], 3)
    
    def test_performance_metrics_creation(self):
        """Test performance metrics creation."""
        metrics = PerformanceMetrics(
            accuracy=0.95,
            precision_macro=0.94,
            recall_macro=0.93,
            f1_macro=0.935,
            f1_micro=0.95,
            f1_weighted=0.94,
            category_f1_scores={"Newsletter": 0.9, "Work": 0.95},
            category_precisions={"Newsletter": 0.92, "Work": 0.93},
            category_recalls={"Newsletter": 0.88, "Work": 0.97},
            avg_confidence=0.85,
            calibration_error=0.05,
            robustness_score=0.8,
            adversarial_robustness=0.75,
            inference_time_ms=50.0,
            throughput_samples_per_sec=20.0,
            memory_usage_mb=500.0,
            training_time_hours=2.0,
            training_steps=10000,
            convergence_steps=8000,
            peak_memory_mb=600.0,
            avg_cpu_usage=75.0,
            total_compute_hours=2.5
        )
        
        self.assertEqual(metrics.accuracy, 0.95)
        self.assertEqual(metrics.f1_macro, 0.935)
        self.assertEqual(metrics.inference_time_ms, 50.0)
        self.assertIn("Newsletter", metrics.category_f1_scores)
    
    def test_model_comparison(self):
        """Test model comparison functionality."""
        # Create target metrics
        target_metrics = PerformanceMetrics(
            accuracy=0.96, precision_macro=0.95, recall_macro=0.94, f1_macro=0.945,
            f1_micro=0.96, f1_weighted=0.95,
            category_f1_scores={}, category_precisions={}, category_recalls={},
            avg_confidence=0.85, calibration_error=0.03,
            robustness_score=0.85, adversarial_robustness=0.8,
            inference_time_ms=60.0, throughput_samples_per_sec=16.7,
            memory_usage_mb=600.0, training_time_hours=3.0,
            training_steps=12000, convergence_steps=10000,
            peak_memory_mb=700.0, avg_cpu_usage=80.0, total_compute_hours=3.5
        )
        
        # Create baseline metrics
        baseline_metrics = {
            "Random Baseline": PerformanceMetrics(
                accuracy=0.1, precision_macro=0.1, recall_macro=0.1, f1_macro=0.1,
                f1_micro=0.1, f1_weighted=0.1,
                category_f1_scores={}, category_precisions={}, category_recalls={},
                avg_confidence=0.5, calibration_error=0.4,
                robustness_score=None, adversarial_robustness=None,
                inference_time_ms=1.0, throughput_samples_per_sec=1000.0,
                memory_usage_mb=1.0, training_time_hours=0.0,
                training_steps=0, convergence_steps=0,
                peak_memory_mb=1.0, avg_cpu_usage=5.0, total_compute_hours=0.0
            )
        }
        
        # Test comparison
        comparisons = self.benchmarking_system._compare_models(
            "Target Model", target_metrics, baseline_metrics
        )
        
        self.assertEqual(len(comparisons), 1)
        comparison = comparisons[0]
        
        self.assertEqual(comparison.model_name, "Target Model")
        self.assertEqual(comparison.baseline_name, "Random Baseline")
        self.assertGreater(comparison.accuracy_improvement, 0.8)  # Huge improvement over random
        self.assertGreater(comparison.f1_improvement, 0.8)
        self.assertGreater(comparison.performance_score, 0.8)  # Should be high
    
    def test_rankings_generation(self):
        """Test model rankings generation."""
        # Create metrics for multiple models
        all_metrics = {
            "Target Model": PerformanceMetrics(
                accuracy=0.95, precision_macro=0.94, recall_macro=0.93, f1_macro=0.935,
                f1_micro=0.95, f1_weighted=0.94,
                category_f1_scores={}, category_precisions={}, category_recalls={},
                avg_confidence=0.85, calibration_error=0.05,
                robustness_score=0.8, adversarial_robustness=0.75,
                inference_time_ms=50.0, throughput_samples_per_sec=20.0,
                memory_usage_mb=500.0, training_time_hours=2.0,
                training_steps=10000, convergence_steps=8000,
                peak_memory_mb=600.0, avg_cpu_usage=75.0, total_compute_hours=2.5
            ),
            "Baseline Model": PerformanceMetrics(
                accuracy=0.85, precision_macro=0.84, recall_macro=0.83, f1_macro=0.835,
                f1_micro=0.85, f1_weighted=0.84,
                category_f1_scores={}, category_precisions={}, category_recalls={},
                avg_confidence=0.75, calibration_error=0.1,
                robustness_score=0.7, adversarial_robustness=0.65,
                inference_time_ms=30.0, throughput_samples_per_sec=33.3,
                memory_usage_mb=200.0, training_time_hours=1.0,
                training_steps=5000, convergence_steps=4000,
                peak_memory_mb=250.0, avg_cpu_usage=60.0, total_compute_hours=1.2
            )
        }
        
        rankings = self.benchmarking_system._generate_rankings(all_metrics)
        
        # Check ranking structure
        self.assertIn("accuracy", rankings)
        self.assertIn("efficiency", rankings)
        self.assertIn("overall", rankings)
        
        # Target model should rank higher in accuracy
        accuracy_ranking = rankings["accuracy"]
        self.assertEqual(accuracy_ranking[0][0], "Target Model")  # First in accuracy
        self.assertEqual(accuracy_ranking[1][0], "Baseline Model")  # Second in accuracy
        
        # Baseline might rank higher in efficiency (faster, less memory)
        efficiency_ranking = rankings["efficiency"]
        self.assertEqual(len(efficiency_ranking), 2)
    
    def test_benchmarking_analysis(self):
        """Test benchmarking result analysis."""
        # Create mock metrics and comparisons
        target_metrics = PerformanceMetrics(
            accuracy=0.96, precision_macro=0.95, recall_macro=0.94, f1_macro=0.945,
            f1_micro=0.96, f1_weighted=0.95,
            category_f1_scores={}, category_precisions={}, category_recalls={},
            avg_confidence=0.85, calibration_error=0.03,
            robustness_score=0.85, adversarial_robustness=0.8,
            inference_time_ms=60.0, throughput_samples_per_sec=16.7,
            memory_usage_mb=600.0, training_time_hours=3.0,
            training_steps=12000, convergence_steps=10000,
            peak_memory_mb=700.0, avg_cpu_usage=80.0, total_compute_hours=3.5
        )
        
        baseline_metrics = {}
        comparisons = []
        
        # Analyze results
        analysis = self.benchmarking_system._analyze_benchmarking_results(
            "Target Model", target_metrics, baseline_metrics, comparisons
        )
        
        # Check analysis structure
        self.assertIn("summary", analysis)
        self.assertIn("findings", analysis)
        self.assertIn("recommendations", analysis)
        self.assertIn("resource_analysis", analysis)
        self.assertIn("scalability_analysis", analysis)
        
        # Check that 95%+ accuracy is recognized
        self.assertIn("‚úì Achieved 95%+ accuracy target", analysis["findings"])
        
        # Check resource analysis structure
        resource_analysis = analysis["resource_analysis"]
        self.assertIn("memory_efficiency", resource_analysis)
        self.assertIn("compute_efficiency", resource_analysis)
        
        # Check scalability analysis
        scalability_analysis = analysis["scalability_analysis"]
        self.assertIn("inference_scalability", scalability_analysis)
        self.assertIn("training_scalability", scalability_analysis)


class TestIntegrationScenarios(unittest.TestCase):
    """Integration tests for validation and benchmarking systems."""
    
    def setUp(self):
        """Set up integration test fixtures."""
        self.temp_dir = tempfile.mkdtemp()
    
    def tearDown(self):
        """Clean up integration test fixtures."""
        import shutil
        shutil.rmtree(self.temp_dir, ignore_errors=True)
    
    def test_validation_to_benchmarking_pipeline(self):
        """Test pipeline from validation to benchmarking."""
        # Create mock validation result
        validation_result = ComprehensiveValidationResult(
            validation_id="test_validation",
            timestamp=datetime.now(),
            config=AccuracyValidationConfig(dataset_paths=["test"]),
            total_experiments=5,
            successful_experiments=4,
            failed_experiments=1,
            experiment_results=[],
            accuracy_target_achievement_rate=0.8,
            category_target_achievement_rate=0.9,
            experiments_meeting_both_targets=3,
            best_overall_accuracy=0.96,
            average_accuracy=0.93,
            accuracy_std=0.02,
            category_performance_stats={},
            language_performance={},
            strategy_performance={},
            model_config_performance={},
            total_validation_time=3600.0,
            average_training_time=900.0,
            average_inference_time=50.0,
            calibration_quality={},
            uncertainty_analysis={},
            validation_summary="Test validation completed",
            recommendations=["Improve model architecture"],
            output_files=[]
        )
        
        # Create mock robustness result
        robustness_result = ComprehensiveRobustnessResult(
            robustness_id="test_robustness",
            timestamp=datetime.now(),
            config=RobustnessTestConfig(output_dir=self.temp_dir),
            total_tests=10,
            passed_tests=8,
            failed_tests=2,
            test_results=[],
            overall_robustness_score=0.85,
            worst_case_degradation=0.15,
            average_degradation=0.05,
            format_robustness={},
            domain_robustness={},
            adversarial_robustness={},
            noise_robustness={},
            length_robustness={},
            most_vulnerable_categories=[],
            most_robust_categories=[],
            robustness_summary="Test robustness completed",
            improvement_recommendations=["Add data augmentation"],
            output_files=[]
        )
        
        # Create benchmarking system
        benchmarking_system = PerformanceBenchmarkingSystem(output_dir=self.temp_dir)
        
        # Test that validation and robustness results can be used in benchmarking
        # (This would normally be part of execute_comprehensive_benchmarking)
        
        # Verify that the results contain expected information
        self.assertEqual(validation_result.best_overall_accuracy, 0.96)
        self.assertEqual(robustness_result.overall_robustness_score, 0.85)
        
        # Test that benchmarking system can extract metrics from these results
        # This simulates what _evaluate_target_model would do
        extracted_accuracy = validation_result.best_overall_accuracy
        extracted_robustness = robustness_result.overall_robustness_score
        
        self.assertGreaterEqual(extracted_accuracy, 0.95)  # Meets target
        self.assertGreater(extracted_robustness, 0.8)  # Good robustness
    
    def test_end_to_end_validation_workflow(self):
        """Test end-to-end validation workflow components."""
        # Test that all systems can be initialized together
        validation_config = AccuracyValidationConfig(
            dataset_paths=["test_dataset"],
            output_dir=os.path.join(self.temp_dir, "validation")
        )
        
        robustness_config = RobustnessTestConfig(
            output_dir=os.path.join(self.temp_dir, "robustness")
        )
        
        validation_system = AccuracyValidationSystem(validation_config)
        robustness_system = RobustnessTestingSystem(robustness_config)
        benchmarking_system = PerformanceBenchmarkingSystem(
            output_dir=os.path.join(self.temp_dir, "benchmarking")
        )
        
        # Verify all systems are properly initialized
        self.assertIsNotNone(validation_system.orchestrator)
        self.assertIsNotNone(robustness_system.variation_generator)
        self.assertIsNotNone(benchmarking_system.baseline_implementation)
        
        # Test that output directories are created
        self.assertTrue(Path(validation_config.output_dir).exists())
        self.assertTrue(Path(robustness_config.output_dir).exists())
        self.assertTrue(Path(benchmarking_system.output_dir).exists())
    
    def test_error_handling_and_recovery(self):
        """Test error handling and recovery mechanisms."""
        # Test validation system with invalid configuration
        with self.assertRaises(Exception):
            invalid_config = AccuracyValidationConfig(
                dataset_paths=[],  # Empty dataset paths should cause issues
                output_dir=self.temp_dir
            )
            validation_system = AccuracyValidationSystem(invalid_config)
            # This should fail when trying to generate experiment configurations
            validation_system._generate_experiment_configurations()
        
        # Test robustness system with invalid samples
        robustness_config = RobustnessTestConfig(output_dir=self.temp_dir)
        robustness_system = RobustnessTestingSystem(robustness_config)
        
        # Test with empty dataset
        empty_dataset = []
        sampled = robustness_system._sample_test_data(empty_dataset)
        self.assertEqual(len(sampled), 0)  # Should handle empty gracefully
        
        # Test benchmarking system with invalid metrics
        benchmarking_system = PerformanceBenchmarkingSystem(output_dir=self.temp_dir)
        
        # Should handle empty baseline metrics gracefully
        empty_baselines = {}
        rankings = benchmarking_system._generate_rankings(empty_baselines)
        
        self.assertIn("accuracy", rankings)
        self.assertEqual(len(rankings["accuracy"]), 0)


def run_validation_and_benchmarking_tests():
    """Run all validation and benchmarking tests."""
    
    # Create test suite
    test_suite = unittest.TestSuite()
    
    # Add test cases
    test_suite.addTest(unittest.makeSuite(TestAccuracyValidationSystem))
    test_suite.addTest(unittest.makeSuite(TestRobustnessTestingSystem))
    test_suite.addTest(unittest.makeSuite(TestPerformanceBenchmarkingSystem))
    test_suite.addTest(unittest.makeSuite(TestIntegrationScenarios))
    
    # Run tests
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(test_suite)
    
    # Return test results
    return {
        "tests_run": result.testsRun,
        "failures": len(result.failures),
        "errors": len(result.errors),
        "success_rate": (result.testsRun - len(result.failures) - len(result.errors)) / result.testsRun if result.testsRun > 0 else 0.0,
        "details": {
            "failures": [str(failure) for failure in result.failures],
            "errors": [str(error) for error in result.errors]
        }
    }


if __name__ == "__main__":
    print("Running Validation and Benchmarking Tests...")
    print("=" * 60)
    
    # Run tests
    test_results = run_validation_and_benchmarking_tests()
    
    # Print summary
    print("\n" + "=" * 60)
    print("TEST RESULTS SUMMARY")
    print("=" * 60)
    print(f"Tests Run: {test_results['tests_run']}")
    print(f"Failures: {test_results['failures']}")
    print(f"Errors: {test_results['errors']}")
    print(f"Success Rate: {test_results['success_rate']:.1%}")
    
    if test_results['failures'] > 0:
        print("\nFAILURES:")
        for failure in test_results['details']['failures']:
            print(f"  - {failure}")
    
    if test_results['errors'] > 0:
        print("\nERRORS:")
        for error in test_results['details']['errors']:
            print(f"  - {error}")
    
    # Overall status
    if test_results['success_rate'] >= 0.9:
        print("\n‚úÖ VALIDATION AND BENCHMARKING TESTS PASSED")
    elif test_results['success_rate'] >= 0.7:
        print("\n‚ö†Ô∏è  VALIDATION AND BENCHMARKING TESTS MOSTLY PASSED")
    else:
        print("\n‚ùå VALIDATION AND BENCHMARKING TESTS FAILED")
    
    print("\nAll validation and benchmarking systems are ready for use!")