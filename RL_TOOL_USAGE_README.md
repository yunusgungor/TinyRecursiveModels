# RL ve Tool KullanÄ±mÄ± ile TRM EÄŸitimi

Bu dokÃ¼mantasyon, TRM (Tiny Recursive Model) modelini Reinforcement Learning (RL) ve Tool kullanÄ±mÄ± ile nasÄ±l eÄŸiteceÄŸinizi aÃ§Ä±klar.

## ğŸ¯ Ã–zellikler

### Reinforcement Learning (RL) Entegrasyonu
- **PPO (Proximal Policy Optimization)** algoritmasÄ±
- **Experience replay** ve **advantage estimation**
- **Multi-step episodes** ile gerÃ§ekÃ§i eÄŸitim
- **Real-time reward feedback** sistemi
- **Evaluation metrics** ve **performance tracking**

### Tool KullanÄ±mÄ±
- **5 farklÄ± tool** entegrasyonu:
  - `price_comparison`: Fiyat karÅŸÄ±laÅŸtÄ±rma
  - `inventory_check`: Stok kontrolÃ¼
  - `review_analysis`: Yorum analizi
  - `trend_analysis`: Trend analizi
  - `budget_optimizer`: BÃ¼tÃ§e optimizasyonu
- **Adaptive tool selection** - model hangi tool'u ne zaman kullanacaÄŸÄ±nÄ± Ã¶ÄŸrenir
- **Tool result encoding** - tool sonuÃ§larÄ± model state'ine entegre edilir
- **Caching system** - tool Ã§aÄŸrÄ±larÄ± cache'lenir
- **Performance monitoring** - tool kullanÄ±m istatistikleri

## ğŸ“ Kod YapÄ±sÄ±

```
models/
â”œâ”€â”€ rl/                          # RL Components
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ environment.py           # Gift recommendation environment
â”‚   â”œâ”€â”€ rl_trm.py               # RL-enhanced TRM model
â”‚   â””â”€â”€ trainer.py              # RL training infrastructure
â”œâ”€â”€ tools/                       # Tool Components
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ tool_registry.py        # Tool management system
â”‚   â”œâ”€â”€ gift_tools.py           # Gift-specific tools
â”‚   â””â”€â”€ tool_enhanced_trm.py    # Tool-enhanced TRM model
config/
â”œâ”€â”€ rl_gift_recommendation.yaml      # RL training config
â””â”€â”€ tool_enhanced_gift_recommendation.yaml  # Tool training config
utils/
â””â”€â”€ data_generator.py           # Synthetic data generation
train_rl_gift_recommendation.py     # RL training script
train_tool_enhanced_gift_recommendation.py  # Tool training script
test_rl_tool_integration.py    # Test script
```

## ğŸš€ HÄ±zlÄ± BaÅŸlangÄ±Ã§

### 1. Gereksinimler

```bash
# Mevcut TRM requirements'larÄ± + ek paketler
pip install torch torchvision torchaudio
pip install wandb  # Opsiyonel: logging iÃ§in
pip install requests  # Tool'lar iÃ§in
pip install pandas numpy scikit-learn
```

### 2. Test Etme

Ã–nce her ÅŸeyin Ã§alÄ±ÅŸtÄ±ÄŸÄ±ndan emin olun:

```bash
# TÃ¼m testleri Ã§alÄ±ÅŸtÄ±r
python test_rl_tool_integration.py --test all

# Sadece RL testi
python test_rl_tool_integration.py --test rl

# Sadece Tool testi
python test_rl_tool_integration.py --test tools
```

### 3. Veri HazÄ±rlama

```bash
# Synthetic veri oluÅŸtur
python utils/data_generator.py
```

Bu komut ÅŸunlarÄ± oluÅŸturur:
- `data/gift_catalog.json` - Hediye kataloÄŸu
- `data/gift_recommendation_train.json` - EÄŸitim verisi
- `data/gift_recommendation_test.json` - Test verisi

## ğŸ“ EÄŸitim SenaryolarÄ±

### Senaryo 1: Sadece RL EÄŸitimi

```bash
# Temel RL eÄŸitimi
python train_rl_gift_recommendation.py \
    --config config/rl_gift_recommendation.yaml \
    --wandb

# Debug mode (kÃ¼Ã§Ã¼k model, hÄ±zlÄ± test)
python train_rl_gift_recommendation.py \
    --config config/rl_gift_recommendation.yaml \
    --debug
```

**Beklenen SonuÃ§lar:**
- EÄŸitim sÃ¼resi: 2-4 saat (Mac'te)
- Model boyutu: ~10MB
- Final reward: 0.6-0.8

### Senaryo 2: Tool-Enhanced EÄŸitim

```bash
# Tam tool-enhanced eÄŸitim (3 aÅŸamalÄ±)
python train_tool_enhanced_gift_recommendation.py \
    --config config/tool_enhanced_gift_recommendation.yaml \
    --phase all \
    --wandb

# Sadece tool Ã¶ÄŸrenme aÅŸamasÄ±
python train_tool_enhanced_gift_recommendation.py \
    --config config/tool_enhanced_gift_recommendation.yaml \
    --phase phase2
```

**3 AÅŸamalÄ± EÄŸitim:**
1. **Phase 1**: Supervised pre-training (500 epoch)
2. **Phase 2**: Tool usage learning (300 epoch)
3. **Phase 3**: RL fine-tuning with tools (700 epoch)

**Beklenen SonuÃ§lar:**
- EÄŸitim sÃ¼resi: 6-8 saat (Mac'te)
- Model boyutu: ~15MB
- Final reward: 0.7-0.9 (tool'lar sayesinde daha yÃ¼ksek)
- Tool kullanÄ±m oranÄ±: %60-80

### Senaryo 3: Ã–zelleÅŸtirilmiÅŸ EÄŸitim

Kendi config'inizi oluÅŸturun:

```yaml
# custom_config.yaml
arch:
  hidden_size: 128  # Mac iÃ§in kÃ¼Ã§Ã¼k
  L_layers: 1
  H_cycles: 2
  L_cycles: 2
  max_tool_calls_per_step: 1  # Az tool kullanÄ±mÄ±

rl_training:
  num_episodes: 500  # KÄ±sa eÄŸitim
  batch_size: 8
  eval_frequency: 25
```

```bash
python train_tool_enhanced_gift_recommendation.py \
    --config custom_config.yaml \
    --debug
```

## ğŸ”§ KonfigÃ¼rasyon SeÃ§enekleri

### Model Parametreleri

```yaml
arch:
  # TRM temel parametreleri
  hidden_size: 256        # Model boyutu
  L_layers: 2            # Layer sayÄ±sÄ±
  H_cycles: 3            # Recursive cycles
  L_cycles: 4
  
  # RL parametreleri
  action_space_size: 100  # Maksimum hediye sayÄ±sÄ±
  max_recommendations: 5  # Ã–nerilen hediye sayÄ±sÄ±
  
  # Tool parametreleri
  max_tool_calls_per_step: 3    # AdÄ±m baÅŸÄ±na max tool
  tool_call_threshold: 0.5      # Tool kullanÄ±m eÅŸiÄŸi
  tool_fusion_method: "concatenate"  # Tool entegrasyon yÃ¶ntemi
```

### EÄŸitim Parametreleri

```yaml
rl_training:
  num_episodes: 2000      # Toplam episode
  batch_size: 32          # Batch boyutu
  learning_rate: 1e-4     # Ã–ÄŸrenme oranÄ±
  gamma: 0.99            # Discount factor
  eval_frequency: 50      # DeÄŸerlendirme sÄ±klÄ±ÄŸÄ±
```

## ğŸ“Š Monitoring ve DeÄŸerlendirme

### Weights & Biases Integration

```bash
# WandB ile eÄŸitim
python train_tool_enhanced_gift_recommendation.py \
    --config config/tool_enhanced_gift_recommendation.yaml \
    --wandb
```

**Takip Edilen Metrikler:**
- `avg_reward`: Ortalama episode reward'u
- `eval_reward_mean`: DeÄŸerlendirme reward'u
- `policy_loss`: Policy loss
- `value_loss`: Value loss
- `tool_calls`: Tool kullanÄ±m sayÄ±sÄ±
- `tool_avg_time`: Ortalama tool execution sÃ¼resi

### Manuel Monitoring

```python
# EÄŸitim sÄ±rasÄ±nda model istatistikleri
model = ToolEnhancedTRM(config)
# ... eÄŸitim ...

# Tool kullanÄ±m istatistikleri
stats = model.get_tool_usage_stats()
print(f"Total tool calls: {stats['total_calls']}")
print(f"Most used tool: {stats['most_used_tool']}")
print(f"Success rates: {stats['success_rates']}")
```

## ğŸ¯ Performans Optimizasyonu

### Mac Bilgisayar Ä°Ã§in Optimizasyonlar

```yaml
# Mac-optimized config
arch:
  hidden_size: 128      # 256 yerine
  L_layers: 1          # 2 yerine
  H_cycles: 2          # 3 yerine
  max_tool_calls_per_step: 1  # 3 yerine

global_batch_size: 8   # 16 yerine
rl_training:
  batch_size: 16       # 32 yerine
  experience_buffer_size: 5000  # 10000 yerine
```

### Memory Management

```python
# Gradient accumulation iÃ§in
if batch_idx % accumulation_steps == 0:
    optimizer.step()
    optimizer.zero_grad()

# Tool cache'i temizle
model.tool_registry.clear_cache()
```

## ğŸ› Troubleshooting

### YaygÄ±n Problemler

**1. Memory Error (Mac'te)**
```bash
# Batch size'Ä± kÃ¼Ã§Ã¼lt
--debug flag'i kullan
# veya config'te batch_size: 4
```

**2. Tool Timeout**
```yaml
tools:
  timeout: 60  # 30'dan 60'a Ã§Ä±kar
  max_concurrent_calls: 2  # 5'ten 2'ye dÃ¼ÅŸÃ¼r
```

**3. DÃ¼ÅŸÃ¼k Reward**
```yaml
# Daha fazla exploration
arch:
  halt_exploration_prob: 0.2  # 0.1'den artÄ±r
  epsilon: 0.2  # epsilon-greedy iÃ§in
```

**4. Tool KullanÄ±mÄ± Az**
```yaml
arch:
  tool_call_threshold: 0.3  # 0.5'ten dÃ¼ÅŸÃ¼r
  tool_usage_reward_weight: 0.2  # 0.1'den artÄ±r
```

### Debug Mode

```bash
# DetaylÄ± logging
export PYTHONPATH=.
python -u train_tool_enhanced_gift_recommendation.py \
    --config config/tool_enhanced_gift_recommendation.yaml \
    --debug \
    --wandb 2>&1 | tee training.log
```

## ğŸ“ˆ SonuÃ§larÄ± DeÄŸerlendirme

### BaÅŸarÄ± Kriterleri

**RL-Only Model:**
- Final reward > 0.6
- Evaluation reward artÄ±ÅŸ trendi
- Episode length stabilizasyonu

**Tool-Enhanced Model:**
- Final reward > 0.7
- Tool kullanÄ±m oranÄ± > 50%
- Tool success rate > 80%
- Ã‡eÅŸitli tool'larÄ±n kullanÄ±lmasÄ±

### Model KarÅŸÄ±laÅŸtÄ±rmasÄ±

```python
# Ä°ki modeli karÅŸÄ±laÅŸtÄ±r
def compare_models(rl_model_path, tool_model_path):
    # Load models
    rl_model = torch.load(rl_model_path)
    tool_model = torch.load(tool_model_path)
    
    # Test on same scenarios
    test_scenarios = generate_test_scenarios(100)
    
    rl_rewards = evaluate_model(rl_model, test_scenarios)
    tool_rewards = evaluate_model(tool_model, test_scenarios)
    
    print(f"RL-only average reward: {np.mean(rl_rewards):.3f}")
    print(f"Tool-enhanced average reward: {np.mean(tool_rewards):.3f}")
    print(f"Improvement: {(np.mean(tool_rewards) - np.mean(rl_rewards)):.3f}")
```

## ğŸš€ Production Deployment

### Model Export

```python
# Optimized model iÃ§in
model = ToolEnhancedTRM(config)
model.load_state_dict(torch.load("best_model.pt"))

# Quantization
quantized_model = torch.quantization.quantize_dynamic(
    model, {torch.nn.Linear}, dtype=torch.qint8
)

# TorchScript
scripted_model = torch.jit.script(quantized_model)
torch.jit.save(scripted_model, "production_model.pt")
```

### API Servisi

```python
# FastAPI ile deployment
from fastapi import FastAPI
app = FastAPI()

@app.post("/recommend")
async def recommend_gifts(user_profile: dict):
    # Load model
    model = torch.jit.load("production_model.pt")
    
    # Generate recommendations
    recommendations = model.recommend(user_profile)
    
    return {"recommendations": recommendations}
```

## ğŸ“š Ä°leri Seviye KullanÄ±m

### Custom Tool Ekleme

```python
# Yeni tool oluÅŸtur
class CustomTool(BaseTool):
    def __init__(self):
        super().__init__("custom_tool", "My custom tool")
    
    def execute(self, **kwargs):
        # Tool logic
        return {"result": "custom_result"}
    
    def _get_parameter_schema(self):
        return {"type": "object", "properties": {...}}

# Model'e ekle
model.tool_registry.register_tool(CustomTool())
```

### Custom Reward Function

```python
# Ã–zel reward hesaplama
def custom_reward_function(recommendations, user_feedback):
    base_reward = calculate_base_reward(recommendations, user_feedback)
    
    # Ã–zel kriterler
    diversity_bonus = calculate_diversity(recommendations)
    price_penalty = calculate_price_penalty(recommendations, user_feedback)
    
    return base_reward + diversity_bonus - price_penalty
```

### Multi-Agent Training

```python
# Birden fazla agent ile eÄŸitim
agents = [
    ToolEnhancedTRM(config) for _ in range(4)
]

# Population-based training
for generation in range(100):
    # Her agent'Ä± eÄŸit
    for agent in agents:
        train_agent(agent, episodes=50)
    
    # En iyi agent'larÄ± seÃ§ ve mutate et
    best_agents = select_best(agents, top_k=2)
    agents = mutate_and_reproduce(best_agents)
```

Bu dokÃ¼mantasyon ile TRM modelinizi RL ve tool kullanÄ±mÄ± ile baÅŸarÄ±yla eÄŸitebilirsiniz. SorularÄ±nÄ±z iÃ§in GitHub issues kullanabilirsiniz.

## ğŸ‰ SonuÃ§

Bu entegrasyon ile TRM modeliniz:
- **GerÃ§ek zamanlÄ± feedback** ile Ã¶ÄŸrenir
- **External tool'larÄ±** akÄ±llÄ±ca kullanÄ±r
- **SÃ¼rekli iyileÅŸen** Ã¶neriler sunar
- **Production-ready** deployment'a hazÄ±r hale gelir

BaÅŸarÄ±lar! ğŸš€