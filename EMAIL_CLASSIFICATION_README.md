# Email Classification with TRM (Tiny Recursive Reasoning Model)

Bu proje, PRD'de belirtilen AkÄ±llÄ± E-posta DÃ¼zenleyici iÃ§in TRM (Tiny Recursive Reasoning Model) kullanarak e-posta sÄ±nÄ±flandÄ±rma sistemi geliÅŸtirmektedir.

## ğŸ¯ Proje Hedefi

PRD'de belirtilen %95+ doÄŸruluk hedefine ulaÅŸmak iÃ§in TRM modelini e-posta sÄ±nÄ±flandÄ±rma gÃ¶revine adapte etmek ve eÄŸitmek.

## ğŸ“‹ E-posta Kategorileri

Sistem aÅŸaÄŸÄ±daki 10 kategoriyi desteklemektedir:

1. **Newsletter** - BÃ¼ltenler ve haber mektuplarÄ±
2. **Work** - Ä°ÅŸ ile ilgili e-postalar
3. **Personal** - KiÅŸisel e-postalar
4. **Spam** - Ä°stenmeyen e-postalar
5. **Promotional** - Promosyon ve reklam e-postalarÄ±
6. **Social** - Sosyal medya bildirimleri
7. **Finance** - Finansal bildirimler
8. **Travel** - Seyahat ile ilgili e-postalar
9. **Shopping** - AlÄ±ÅŸveriÅŸ bildirimleri
10. **Other** - DiÄŸer kategoriler

## ğŸ—ï¸ Mimari Ã–zellikleri

### TRM Model Adaptasyonu
- **Recursive Reasoning**: E-posta iÃ§eriÄŸini iteratif olarak analiz eder
- **Adaptive Computation Time (ACT)**: Dinamik durma mekanizmasÄ±
- **Parameter Efficiency**: Sadece 7M parametre ile yÃ¼ksek performans
- **Classification Head**: E-posta kategorileri iÃ§in Ã¶zel sÄ±nÄ±flandÄ±rma katmanÄ±

### Teknik Ã–zellikler
- **Vocabulary Size**: 5000 token (dinamik olarak ayarlanÄ±r)
- **Sequence Length**: 512 token (e-posta uzunluÄŸuna gÃ¶re)
- **Hidden Size**: 256-512 (konfigÃ¼rasyona gÃ¶re)
- **Reasoning Cycles**: H_cycles=2, L_cycles=3-4
- **Position Encoding**: RoPE (Rotary Position Embedding)

## ğŸš€ Kurulum ve KullanÄ±m

### 1. Gereksinimler

```bash
pip install -r requirements.txt
```

Temel gereksinimler:
- PyTorch >= 1.12
- transformers
- scikit-learn
- numpy
- pandas
- wandb (opsiyonel, eÄŸitim takibi iÃ§in)

### 2. Veri HazÄ±rlama

#### Ã–rnek Veri ile Test
```bash
python run_email_training.py --sample-data --max-steps 1000
```

#### Kendi Veriniz ile
E-posta verilerinizi JSON formatÄ±nda hazÄ±rlayÄ±n:

```json
[
  {
    "id": "email_001",
    "subject": "Weekly Newsletter - Tech Updates",
    "body": "Here are the latest tech updates...",
    "sender": "newsletter@techblog.com",
    "recipient": "user@example.com",
    "category": "newsletter"
  }
]
```

Veri setini oluÅŸturun:
```bash
python dataset/build_email_dataset.py \
    --input_file data/emails.json \
    --output_dir data/email-classification \
    --num_aug 100 \
    --max_seq_len 512
```

### 3. Model EÄŸitimi

#### Tek GPU ile EÄŸitim
```bash
python train_email_classifier.py \
    data_paths=[data/email-classification] \
    training.max_steps=10000 \
    training.batch_size=32
```

#### Ã‡oklu GPU ile EÄŸitim
```bash
torchrun --nproc-per-node 4 train_email_classifier.py \
    data_paths=[data/email-classification] \
    training.max_steps=10000 \
    training.batch_size=128
```

#### Tam Pipeline
```bash
python run_email_training.py \
    --num-gpus 4 \
    --batch-size 128 \
    --max-steps 10000
```

### 4. Model DeÄŸerlendirme

```python
from models.recursive_reasoning.trm_email import EmailTRM
from evaluators.email import evaluate_email_model
import torch

# Model yÃ¼kleme
checkpoint = torch.load('outputs/email_classification/best_model.pt')
model = EmailTRM(checkpoint['config']['arch'])
model.load_state_dict(checkpoint['model_state_dict'])

# DeÄŸerlendirme
metrics = evaluate_email_model(model, test_dataloader, device)
print(f"Accuracy: {metrics['accuracy']:.4f}")
print(f"F1 Score: {metrics['macro_f1']:.4f}")
```

## ğŸ“Š Performans Metrikleri

Model aÅŸaÄŸÄ±daki metriklerle deÄŸerlendirilir:

- **Accuracy**: Genel doÄŸruluk oranÄ±
- **Precision/Recall/F1**: Kategori bazlÄ± performans
- **Macro/Micro/Weighted F1**: FarklÄ± ortalama tÃ¼rleri
- **Confusion Matrix**: Kategori karÄ±ÅŸÄ±klÄ±k matrisi
- **Confidence Metrics**: Model gÃ¼ven skorlarÄ±

## ğŸ”§ KonfigÃ¼rasyon

### Model Parametreleri (`config/arch/trm_email.yaml`)
```yaml
# Model boyutu
hidden_size: 512
num_heads: 8
L_layers: 2

# Recursive reasoning
H_cycles: 2
L_cycles: 4
halt_max_steps: 8

# E-posta Ã¶zgÃ¼
num_email_categories: 10
classification_dropout: 0.1
```

### EÄŸitim Parametreleri (`config/cfg_email_train.yaml`)
```yaml
training:
  max_steps: 10000
  batch_size: 32
  lr: 1e-4

optimizer:
  name: "adamw"
  weight_decay: 0.1

scheduler:
  name: "linear_warmup_cosine"
  warmup_steps: 500
```

## ğŸ“ˆ EÄŸitim Takibi

### Weights & Biases (WandB)
```bash
# WandB ile eÄŸitim takibi
export WANDB_PROJECT="email-classification-trm"
python train_email_classifier.py use_wandb=true
```

### Yerel Loglar
EÄŸitim loglarÄ± ve metrikler `outputs/email_classification/` dizininde saklanÄ±r:
- `best_model.pt`: En iyi model
- `final_metrics.json`: Son deÄŸerlendirme metrikleri
- `confusion_matrix.png`: KarÄ±ÅŸÄ±klÄ±k matrisi grafiÄŸi

## ğŸ›ï¸ Hiperparametre Optimizasyonu

### Ã–nerilen BaÅŸlangÄ±Ã§ DeÄŸerleri
```yaml
# HÄ±zlÄ± test iÃ§in
hidden_size: 256
H_cycles: 2
L_cycles: 3
batch_size: 32
max_steps: 5000

# YÃ¼ksek performans iÃ§in
hidden_size: 512
H_cycles: 3
L_cycles: 6
batch_size: 64
max_steps: 20000
```

### Grid Search Ã–rneÄŸi
```bash
# FarklÄ± learning rate'ler test etme
for lr in 1e-4 5e-5 1e-5; do
    python train_email_classifier.py \
        optimizer.lr=$lr \
        experiment_name="lr_${lr}"
done
```

## ğŸš€ Production Deployment

### Model Inference
```python
import torch
from models.recursive_reasoning.trm_email import EmailTRM

class EmailClassifier:
    def __init__(self, model_path):
        checkpoint = torch.load(model_path)
        self.model = EmailTRM(checkpoint['config']['arch'])
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model.eval()
        
        # Vocabulary ve kategoriler
        self.vocab = checkpoint['vocab']
        self.categories = checkpoint['categories']
    
    def predict(self, email_text):
        # Tokenize email
        tokens = self.tokenize(email_text)
        
        # Model inference
        with torch.no_grad():
            outputs = self.model(tokens)
            prediction = torch.argmax(outputs['logits'], dim=-1)
        
        return self.categories[prediction.item()]
```

### API Servisi
```python
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI()
classifier = EmailClassifier('best_model.pt')

class EmailRequest(BaseModel):
    subject: str
    body: str
    sender: str

@app.post("/classify")
async def classify_email(email: EmailRequest):
    category = classifier.predict(email.dict())
    return {"category": category}
```

## ğŸ“ SonuÃ§lar ve Analiz

### Beklenen Performans
- **Hedef Accuracy**: %95+ (PRD gereksinimi)
- **EÄŸitim SÃ¼resi**: 2-4 saat (4 GPU ile)
- **Model Boyutu**: ~7M parametre
- **Inference HÄ±zÄ±**: <100ms per email

### Performans Ä°yileÅŸtirme Ä°puÃ§larÄ±
1. **Veri ArtÄ±rma**: Daha fazla augmentasyon kullanÄ±n
2. **Sequence Length**: E-posta uzunluÄŸuna gÃ¶re optimize edin
3. **Reasoning Cycles**: KarmaÅŸÄ±k e-postalar iÃ§in artÄ±rÄ±n
4. **Ensemble**: Birden fazla model kombinasyonu

## ğŸ› Sorun Giderme

### YaygÄ±n Sorunlar
1. **CUDA Out of Memory**: Batch size'Ä± azaltÄ±n
2. **DÃ¼ÅŸÃ¼k Accuracy**: Daha fazla eÄŸitim verisi ekleyin
3. **Overfitting**: Dropout ve weight decay artÄ±rÄ±n
4. **Slow Training**: Mixed precision kullanÄ±n

### Debug KomutlarÄ±
```bash
# Model parametrelerini kontrol et
python -c "from models.recursive_reasoning.trm_email import EmailTRM; print(EmailTRM.from_config().num_parameters())"

# Veri setini kontrol et
python -c "from puzzle_dataset import PuzzleDataset; ds = PuzzleDataset(...); print(next(iter(ds)))"
```

## ğŸ“š Referanslar

- [TRM Paper: "Less is More: Recursive Reasoning with Tiny Networks"](https://arxiv.org/abs/2510.04871)
- [Original TRM Repository](https://github.com/AlexiaJM/TinyRecursiveReasoningModel)
- [AkÄ±llÄ± E-posta DÃ¼zenleyici PRD](PRD.md)

## ğŸ¤ KatkÄ±da Bulunma

1. Fork yapÄ±n
2. Feature branch oluÅŸturun (`git checkout -b feature/email-classification`)
3. Commit yapÄ±n (`git commit -am 'Add email classification'`)
4. Push yapÄ±n (`git push origin feature/email-classification`)
5. Pull Request oluÅŸturun

## ğŸ“„ Lisans

Bu proje MIT lisansÄ± altÄ±nda lisanslanmÄ±ÅŸtÄ±r. Detaylar iÃ§in [LICENSE](LICENSE) dosyasÄ±na bakÄ±n.