# RL-Enhanced Gift Recommendation TRM Configuration

# Model architecture
arch:
  name: "models.rl.rl_trm@RLEnhancedTRM"
  loss:
    name: "models.losses.supervised_loss@SupervisedLoss"
  
  # Base TRM parameters
  hidden_size: 256
  L_layers: 2
  H_layers: 2
  H_cycles: 3
  L_cycles: 4
  num_heads: 8
  expansion: 2.0
  pos_encodings: "rope"
  rms_norm_eps: 1e-5
  rope_theta: 10000.0
  
  # ACT parameters
  halt_max_steps: 5
  halt_exploration_prob: 0.1
  no_ACT_continue: true
  
  # RL-specific parameters
  action_space_size: 100
  max_recommendations: 5
  value_head_hidden: 256
  policy_head_hidden: 256
  reward_prediction: true
  reward_head_hidden: 128
  
  # Action selection
  action_selection_method: "top_k"  # "top_k", "sampling", "epsilon_greedy"
  epsilon: 0.1
  temperature: 1.0
  
  # PPO parameters
  ppo_clip_ratio: 0.2
  value_loss_coef: 0.5
  entropy_coef: 0.01
  max_grad_norm: 0.5

# Training parameters
global_batch_size: 16
epochs: 1000
lr: 1e-4
lr_min_ratio: 0.1
lr_warmup_steps: 100
weight_decay: 0.01
beta1: 0.9
beta2: 0.95

# RL Training specific
rl_training:
  num_episodes: 2000
  max_steps_per_episode: 10
  batch_size: 32
  gamma: 0.99
  ppo_epochs: 4
  experience_buffer_size: 10000
  min_experiences_for_update: 100
  eval_frequency: 50
  eval_episodes: 10

# Data paths
data_paths: ["data/gift_recommendation_train"]
data_paths_test: ["data/gift_recommendation_test"]

# Environment
environment:
  gift_catalog_path: "data/gift_catalog.json"
  user_feedback_path: "data/user_feedback.json"

# Evaluation
eval_interval: 100
min_eval_interval: 50
evaluators: []

# Checkpointing
checkpoint_every_eval: true
checkpoint_path: "checkpoints/rl_gift_recommendation"
project_name: "GiftRecommendation-RL-TRM"
run_name: null

# Logging
log_frequency: 10
save_frequency: 100

# Device settings
device: "cpu"  # Use "cuda" if available
forward_dtype: "float32"