#!/usr/bin/env python3
"""
Test script for tool integration improvements
Tests all the new features: tool feedback, tool params, resume training
"""

import torch
import sys
import os

sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from train_integrated_enhanced_model import IntegratedEnhancedTrainer, ToolResultEncoder
from models.tools.integrated_enhanced_trm import create_integrated_enhanced_config
from models.rl.environment import UserProfile


def test_device_handling():
    """Test 1: Device handling in ToolResultEncoder"""
    print("\n" + "="*60)
    print("TEST 1: Device Handling")
    print("="*60)
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    encoder = ToolResultEncoder(hidden_dim=128).to(device)
    
    # Test encoding
    tool_results = {
        'price_comparison': {
            'in_budget': [1, 2, 3],
            'over_budget': [4, 5],
            'average_price': 75.5
        }
    }
    
    encoded = encoder(tool_results, device)
    
    assert encoded.device == device, f"‚ùå Device mismatch: {encoded.device} != {device}"
    assert encoded.shape == (128,), f"‚ùå Shape mismatch: {encoded.shape}"
    
    print(f"‚úÖ Device handling correct: {device}")
    print(f"‚úÖ Encoded shape: {encoded.shape}")
    return True


def test_tool_params_generation():
    """Test 2: Tool parameters generation"""
    print("\n" + "="*60)
    print("TEST 2: Tool Parameters Generation")
    print("="*60)
    
    config = create_integrated_enhanced_config()
    config.update({'batch_size': 1, 'hidden_dim': 128})
    
    trainer = IntegratedEnhancedTrainer(config)
    
    # Create test user
    user = UserProfile(
        age=30,
        hobbies=['technology', 'fitness'],
        relationship='friend',
        budget=150.0,
        occasion='birthday',
        personality_traits=['trendy', 'practical']
    )
    
    # Forward pass
    env_state = trainer.env.reset(user)
    carry = trainer.model.initial_carry({
        "inputs": torch.zeros(1, 10, device=trainer.device),
        "puzzle_identifiers": torch.zeros(1, 1, device=trainer.device)
    })
    
    carry, model_output, selected_tools = trainer.model.forward_with_enhancements(
        carry, env_state, trainer.env.gift_catalog
    )
    
    # Check tool_params
    assert 'tool_params' in model_output, "‚ùå tool_params not in model output"
    
    tool_params = model_output['tool_params']
    print(f"‚úÖ tool_params generated: {list(tool_params.keys())}")
    
    for tool_name, params in tool_params.items():
        print(f"  - {tool_name}: {params}")
    
    return True


def test_tool_feedback():
    """Test 3: Tool feedback integration"""
    print("\n" + "="*60)
    print("TEST 3: Tool Feedback Integration")
    print("="*60)
    
    config = create_integrated_enhanced_config()
    config.update({'batch_size': 1, 'hidden_dim': 128})
    
    trainer = IntegratedEnhancedTrainer(config)
    
    # Create test user
    user = UserProfile(
        age=30,
        hobbies=['technology'],
        relationship='friend',
        budget=150.0,
        occasion='birthday',
        personality_traits=['trendy']
    )
    
    # First forward pass without feedback
    env_state = trainer.env.reset(user)
    carry = trainer.model.initial_carry({
        "inputs": torch.zeros(1, 10, device=trainer.device),
        "puzzle_identifiers": torch.zeros(1, 1, device=trainer.device)
    })
    
    carry1, output1, tools1 = trainer.model.forward_with_enhancements(
        carry, env_state, trainer.env.gift_catalog
    )
    
    # Add tool feedback to carry
    tool_results = {
        'price_comparison': {
            'in_budget': [1, 2, 3],
            'over_budget': [],
            'average_price': 100.0
        }
    }
    
    encoded_feedback = trainer.tool_result_encoder(tool_results, trainer.device)
    carry_with_feedback = {'tool_feedback': encoded_feedback.unsqueeze(0)}
    
    # Second forward pass with feedback
    carry2, output2, tools2 = trainer.model.forward_with_enhancements(
        carry_with_feedback, env_state, trainer.env.gift_catalog
    )
    
    # Check if outputs are different (feedback had an effect)
    diff = torch.abs(output1['predicted_rewards'] - output2['predicted_rewards']).mean()
    
    print(f"‚úÖ Tool feedback integrated into carry state")
    print(f"‚úÖ Reward difference with/without feedback: {diff.item():.6f}")
    
    if diff.item() > 0.001:
        print(f"‚úÖ Feedback has measurable effect on predictions")
    else:
        print(f"‚ö†Ô∏è  Feedback effect is minimal (might need tuning)")
    
    return True


def test_checkpoint_save_load():
    """Test 4: Checkpoint save and load"""
    print("\n" + "="*60)
    print("TEST 4: Checkpoint Save/Load")
    print("="*60)
    
    config = create_integrated_enhanced_config()
    config.update({'batch_size': 1, 'hidden_dim': 128})
    
    # Create and save
    trainer1 = IntegratedEnhancedTrainer(config)
    trainer1.save_model("test_checkpoint.pt", epoch=5, metrics={'test': 0.5})
    
    # Load into new trainer
    trainer2 = IntegratedEnhancedTrainer(config)
    epoch = trainer2.load_model("checkpoints/integrated_enhanced/test_checkpoint.pt")
    
    assert epoch == 5, f"‚ùå Epoch mismatch: {epoch} != 5"
    
    # Compare model weights
    for (name1, param1), (name2, param2) in zip(
        trainer1.model.named_parameters(), 
        trainer2.model.named_parameters()
    ):
        assert torch.allclose(param1, param2), f"‚ùå Model weights differ: {name1}"
    
    # Compare tool encoder weights
    for (name1, param1), (name2, param2) in zip(
        trainer1.tool_result_encoder.named_parameters(),
        trainer2.tool_result_encoder.named_parameters()
    ):
        assert torch.allclose(param1, param2), f"‚ùå Tool encoder weights differ: {name1}"
    
    print(f"‚úÖ Checkpoint saved and loaded successfully")
    print(f"‚úÖ Model weights match")
    print(f"‚úÖ Tool encoder weights match")
    
    # Cleanup
    os.remove("checkpoints/integrated_enhanced/test_checkpoint.pt")
    
    return True


def test_gradient_flow():
    """Test 5: Gradient flow through tool encoder"""
    print("\n" + "="*60)
    print("TEST 5: Gradient Flow")
    print("="*60)
    
    config = create_integrated_enhanced_config()
    config.update({'batch_size': 2, 'hidden_dim': 128})
    
    trainer = IntegratedEnhancedTrainer(config)
    
    # Generate batch
    users, gifts, targets = trainer.generate_training_batch(batch_size=2)
    
    # Forward pass
    batch_outputs = []
    tool_encodings_batch = []
    for user in users:
        env_state = trainer.env.reset(user)
        carry = trainer.model.initial_carry({
            "inputs": torch.zeros(1, 10, device=trainer.device),
            "puzzle_identifiers": torch.zeros(1, 1, device=trainer.device)
        })
        
        carry, model_output, selected_tools = trainer.model.forward_with_enhancements(
            carry, env_state, trainer.env.gift_catalog
        )
        
        # Simulate tool execution
        tool_results = {
            'price_comparison': {
                'in_budget': [1, 2],
                'over_budget': [],
                'average_price': 100.0
            }
        }
        
        # Encode tool results - this needs to be part of computation graph
        encoded = trainer.tool_result_encoder(tool_results, trainer.device)
        tool_encodings_batch.append(encoded)
        
        model_output['tool_results'] = tool_results
        batch_outputs.append(model_output)
    
    # Stack outputs
    stacked_outputs = {}
    for key in batch_outputs[0].keys():
        if isinstance(batch_outputs[0][key], torch.Tensor):
            stacked_outputs[key] = torch.stack([output[key] for output in batch_outputs])
        else:
            stacked_outputs[key] = [output[key] for output in batch_outputs]
    
    # Stack tool encodings and add to outputs (this ensures gradient flow)
    if tool_encodings_batch:
        stacked_tool_encodings = torch.stack(tool_encodings_batch)
        # Add a simple loss component that uses tool encodings
        tool_encoding_loss = stacked_tool_encodings.mean()  # Simple mean as auxiliary loss
    
    # Compute loss
    loss, loss_components = trainer.compute_enhanced_loss(stacked_outputs, targets)
    
    # Add tool encoding loss to ensure gradient flow through encoder
    if tool_encodings_batch:
        loss = loss + 0.01 * tool_encoding_loss  # Small weight to not dominate
    
    # Backward
    trainer.optimizer.zero_grad()
    loss.backward()
    
    # Check gradients
    model_has_grad = any(p.grad is not None for p in trainer.model.parameters())
    encoder_has_grad = any(p.grad is not None for p in trainer.tool_result_encoder.parameters())
    
    assert model_has_grad, "‚ùå Model has no gradients"
    assert encoder_has_grad, "‚ùå Tool encoder has no gradients"
    
    print(f"‚úÖ Model gradients computed")
    print(f"‚úÖ Tool encoder gradients computed")
    print(f"‚úÖ Loss: {loss.item():.4f}")
    print(f"   - Category loss: {loss_components.get('category_loss', 0):.4f}")
    print(f"   - Tool loss: {loss_components.get('tool_loss', 0):.4f}")
    print(f"   - Reward loss: {loss_components.get('reward_loss', 0):.4f}")
    
    return True


def run_all_tests():
    """Run all tests"""
    print("\n" + "üß™ " + "="*58)
    print("üß™ TOOL INTEGRATION TEST SUITE")
    print("üß™ " + "="*58)
    
    tests = [
        ("Device Handling", test_device_handling),
        ("Tool Parameters Generation", test_tool_params_generation),
        ("Tool Feedback Integration", test_tool_feedback),
        ("Checkpoint Save/Load", test_checkpoint_save_load),
        ("Gradient Flow", test_gradient_flow)
    ]
    
    results = []
    for test_name, test_func in tests:
        try:
            success = test_func()
            results.append((test_name, success, None))
        except Exception as e:
            results.append((test_name, False, str(e)))
            print(f"\n‚ùå {test_name} FAILED: {e}")
    
    # Summary
    print("\n" + "="*60)
    print("TEST SUMMARY")
    print("="*60)
    
    passed = sum(1 for _, success, _ in results if success)
    total = len(results)
    
    for test_name, success, error in results:
        status = "‚úÖ PASS" if success else "‚ùå FAIL"
        print(f"{status}: {test_name}")
        if error:
            print(f"       Error: {error}")
    
    print(f"\n{passed}/{total} tests passed")
    
    if passed == total:
        print("\nüéâ ALL TESTS PASSED! üéâ")
        return True
    else:
        print(f"\n‚ö†Ô∏è  {total - passed} test(s) failed")
        return False


if __name__ == "__main__":
    success = run_all_tests()
    sys.exit(0 if success else 1)
